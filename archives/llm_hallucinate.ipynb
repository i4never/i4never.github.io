{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe539b20",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "基于统计学中对模型输出不确定性估计理论，在beam search过程中，通过采样，在生成过程中计算token的不确定性，作为token真实性的参考指标。\n",
    "\n",
    "------\n",
    "\n",
    "LLM建模在自回归语言模型上，不以信息真实性为目的，因此有概率产生虚假信息。LLM之前，在翻译、摘要生成、虚假新闻检测等领域，就有针对生成结果可靠性的相关研究。研究主要又分为如何避免、如何检测两个方向。如何减轻LLM生成虚假信息，同样可以从这两个方面入手：\n",
    "\n",
    "- 在训练过程中干预，主要有引导reward模型判别虚假内容；提高数据质量；进行数据增强等方法。\n",
    "\n",
    "- 在推理过程中干预，通过某种方式判定生成结果的真实性。\n",
    "\n",
    "针对第二点，不少文献基于rough、bleu、perplexity等指标来设计“真实性”指标。但是文本的真实性往往是一字之差，这些sentence、甚至是paragraph级别的指标粒度太粗，因此，我认为应该设计一个基于token粒度的真实性评估方法。以此以基础，可以衍生出干预beam search过程，惩罚token出现概率等措施。\n",
    "\n",
    "### Theory\n",
    "\n",
    "在research中，hallucinate被分为两种情况：\n",
    "\n",
    "- Intrinsic，模型编造见过的事实\n",
    "- Extrinsic，模型编造没见过的事实\n",
    "\n",
    "在uncertainty度量问题中，同样把uncertainty分为两种：\n",
    "\n",
    "- Epistemic，数据集中包含正确信息\n",
    "- Aleatoric，数据集包含噪声\n",
    "\n",
    "Intrinsic和Epistemic非常相似，都是模型在见过的数据集上犯错。在数据集包含正确信息的前提下，beam search过程中可以参考Epistemic uncertainty度量估计的方式，来计算模型对token的不确定性程度，作为“真实性”的某种度量。\n",
    "\n",
    "估计不确定性，最直观的方法是方差，但是beam search时每个token只计算了一次，单个数值无法计算方差，需要设法获取模型每个时间步的不同输出。现代网络结构往往含有dropout层，在训练过程中，dropout下的模型参数可以看作二项分布，推理过程中打开dropout，即可视作是对模型参数的采样。推理过程先关闭dropout，获得时间步$t$的$token_t$，再打开dropout，推理$N$次，网络的$N$个输出，计算$N$个概率分布的熵、熵的方差、$token_t$的概率方差等指标，代表模型对输出的确定程度，某种程度上可以反应$token_t$是否真实。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e498ec0",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "在chatglm-6b上测试\n",
    "\n",
    "主要hack了chatglm-6b的forward过程，添加了dropout层的控制。\n",
    "\n",
    "模拟了一个beam_size=1，不做随机操作的beam search，每个时间步做如下操作：\n",
    "- 1. 关闭dropout\n",
    "- 2. forward，获取token\n",
    "- 3. 打开dropout\n",
    "- 4. 计算N次，获取N个概率分布，基于N个概率分布计算分布的entropy、std(entropy)、avg(entropy)、avg(token_prob)指标，作为token的真实程度估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e47cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from typing import List, Tuple, Optional, Callable, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.generation.logits_process import LogitsProcessor\n",
    "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53dc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2510604c3743bca34059c6d0fcbd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c330a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatglm在推理时没有加入dropout，这里hack forward函数，添加drop层\n",
    "\n",
    "def add_infer_dropout(self, dropout_rate = 0.05):\n",
    "    print(\"add infer dropout to model\")\n",
    "    self.infer_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "def chat_glm_forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
    "        inputs_embeds: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        enable_dropout: bool = False,\n",
    ") -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPast]:\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    if self.gradient_checkpointing and self.training:\n",
    "        if use_cache:\n",
    "            logger.warning_once(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    elif input_ids is not None:\n",
    "        batch_size, seq_length = input_ids.shape[:2]\n",
    "    elif inputs_embeds is not None:\n",
    "        batch_size, seq_length = inputs_embeds.shape[:2]\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "    if past_key_values is None:\n",
    "        if self.pre_seq_len is not None:\n",
    "            past_key_values = self.get_prompt(batch_size=input_ids.shape[0], device=input_ids.device,\n",
    "                                              dtype=inputs_embeds.dtype)\n",
    "        else:\n",
    "            past_key_values = tuple([None] * len(self.layers))\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = self.get_masks(\n",
    "                input_ids,\n",
    "                device=input_ids.device\n",
    "            )\n",
    "\n",
    "\n",
    "        if position_ids is None:\n",
    "            MASK, gMASK = self.config.mask_token_id, self.config.gmask_token_id\n",
    "            seqs = input_ids.tolist()\n",
    "\n",
    "            mask_positions, use_gmasks = [], []\n",
    "            for seq in seqs:\n",
    "                mask_token = gMASK if gMASK in seq else MASK\n",
    "                use_gmask = mask_token == gMASK\n",
    "                mask_positions.append(seq.index(mask_token))\n",
    "                use_gmasks.append(use_gmask)\n",
    "\n",
    "            position_ids = self.get_position_ids(\n",
    "                input_ids,\n",
    "                mask_positions=mask_positions,\n",
    "                device=input_ids.device,\n",
    "                use_gmasks=use_gmasks\n",
    "            )\n",
    "\n",
    "    if self.pre_seq_len is not None and attention_mask is not None:\n",
    "        prefix_attention_mask = torch.ones(batch_size, 1, input_ids.size(-1), self.pre_seq_len).to(\n",
    "            attention_mask.device)\n",
    "        prefix_attention_mask = (prefix_attention_mask < 0.5).bool()\n",
    "        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=3)\n",
    "\n",
    "    # [seq_len, batch, hidden_size]\n",
    "    hidden_states = inputs_embeds.transpose(0, 1)\n",
    "\n",
    "    presents = () if use_cache else None\n",
    "    all_self_attentions = () if output_attentions else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.zeros(1, 1, device=input_ids.device).bool()\n",
    "    else:\n",
    "        attention_mask = attention_mask.to(hidden_states.device)\n",
    "\n",
    "    for i, layer in enumerate(self.layers):\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "        layer_past = past_key_values[i]\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            layer_ret = torch.utils.checkpoint.checkpoint(\n",
    "                layer,\n",
    "                hidden_states,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                torch.tensor(i),\n",
    "                layer_past,\n",
    "                use_cache,\n",
    "                output_attentions\n",
    "            )\n",
    "        else:\n",
    "            layer_ret = layer(\n",
    "                hidden_states,\n",
    "                position_ids=position_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                layer_id=torch.tensor(i),\n",
    "                layer_past=layer_past,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions\n",
    "            )\n",
    "\n",
    "        hidden_states = layer_ret[0]\n",
    "        \n",
    "        if enable_dropout:\n",
    "            hidden_states = self.infer_dropout(hidden_states)\n",
    "            \n",
    "\n",
    "        if use_cache:\n",
    "            presents = presents + (layer_ret[1],)\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attentions = all_self_attentions + (layer_ret[2 if use_cache else 1],)\n",
    "\n",
    "    # Final layer norm.\n",
    "    hidden_states = self.final_layernorm(hidden_states)\n",
    "\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=presents,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attentions,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def chat_glm_for_cd_forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        enable_dropout: bool = False,\n",
    "):\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=position_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        enable_dropout=enable_dropout,\n",
    "    )\n",
    "\n",
    "    hidden_states = transformer_outputs[0]\n",
    "\n",
    "    lm_logits = self.lm_head(hidden_states).permute(1, 0, 2).contiguous()\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "        loss = loss.to(hidden_states.dtype)\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (lm_logits,) + transformer_outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=lm_logits,\n",
    "        past_key_values=transformer_outputs.past_key_values,\n",
    "        hidden_states=transformer_outputs.hidden_states,\n",
    "        attentions=transformer_outputs.attentions,\n",
    "    )\n",
    "\n",
    "\n",
    "type(model.transformer).add_infer_dropout = add_infer_dropout\n",
    "type(model).forward = chat_glm_for_cd_forward\n",
    "type(model.transformer).forward = chat_glm_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b032518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add infer dropout to model\n"
     ]
    }
   ],
   "source": [
    "DROPOUT_RATE = 0.1\n",
    "FORWARD_N = 8\n",
    "\n",
    "model.transformer.add_infer_dropout(DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96136cc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate(prompt, tokenizer, model, forward_n):\n",
    "    # generation params\n",
    "    bos_token_id, eos_token_id = tokenizer.bos_token_id, tokenizer.eos_token_id\n",
    "    max_length = 32\n",
    "    \n",
    "    logits_processor = LogitsProcessorList()\n",
    "    stopping_criteria = StoppingCriteriaList()\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # beam search\n",
    "    batch_size, input_ids_seq_length = input_ids.shape[:2]\n",
    "    \n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "    \n",
    "    token_mc_stddev = list()\n",
    "    token_mc_entropy_avg = list()\n",
    "    token_mc_entropy_stddev = list()\n",
    "    generated_tokens = list()\n",
    "\n",
    "    while len(generated_tokens) < max_length:\n",
    "        print(len(generated_tokens), end=' ')\n",
    "        model_inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "        \n",
    "        # 第一次forward，关闭dropout，计算模型token\n",
    "        model.eval()\n",
    "        outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        next_tokens = torch.argmax(probs, dim=-1)\n",
    "        generated_tokens.append(next_tokens)\n",
    "        \n",
    "        # 采样n次，开启dropout，计算新token的支持概率\n",
    "        step_mc_entropies = list()\n",
    "        step_mc_probs = list()\n",
    "        for j in range(forward_n):\n",
    "            model.train()\n",
    "            outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False,\n",
    "                            enable_dropout=True)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "            probs = nn.functional.softmax(next_token_scores, dim=-1).detach().numpy()\n",
    "#             token_mc_probs[-1][j, 0] = probs[0][next_tokens[0]]\n",
    "            step_mc_entropies.append(-np.sum(probs * np.log2(probs)))\n",
    "            step_mc_probs.append(probs[0][next_tokens[0]])\n",
    "        token_mc_stddev.append(np.var(step_mc_probs))\n",
    "        token_mc_entropy_stddev.append(np.var(step_mc_entropies))\n",
    "        token_mc_entropy_avg.append(np.mean(step_mc_entropies))\n",
    "            \n",
    "        if next_tokens[0] == tokenizer.eos_token_id:\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "\n",
    "    return [tid.numpy() for tid in generated_tokens], token_mc_stddev, token_mc_entropy_avg, token_mc_entropy_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a106cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "def colored_background(r, g, b, text):\n",
    "    return f\"\\033[48;2;{r};{g};{b}m{text}\\033[0m\"\n",
    "    \n",
    "def print_seq(tokens, uncertainty):\n",
    "    l = 150\n",
    "    h = 255\n",
    "    max_ = np.max(uncertainty)\n",
    "    min_ = np.min(uncertainty)\n",
    "    k = (h - l) / (max_ - min_)\n",
    "    for token, v in zip(tokens, l + k * (uncertainty - min_)):\n",
    "        print(colored_background(0, int(v), 0, token), end='')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ebc89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "measure by mc_stddev\n",
      "\u001b[48;2;0;150;0mLionel\u001b[0m\u001b[48;2;0;150;0mMessi\u001b[0m\u001b[48;2;0;150;0m的生日\u001b[0m\u001b[48;2;0;255;0m是\u001b[0m\u001b[48;2;0;161;0m1\u001b[0m\u001b[48;2;0;199;0m9\u001b[0m\u001b[48;2;0;167;0m8\u001b[0m\u001b[48;2;0;150;0m7\u001b[0m\u001b[48;2;0;151;0m年\u001b[0m\u001b[48;2;0;161;0m6\u001b[0m\u001b[48;2;0;199;0m月\u001b[0m\u001b[48;2;0;150;0m2\u001b[0m\u001b[48;2;0;150;0m4\u001b[0m\u001b[48;2;0;192;0m日\u001b[0m\u001b[48;2;0;185;0m。\u001b[0m\u001b[48;2;0;201;0m\u001b[0m\n",
      "measure by mc_enctopy_average\n",
      "\u001b[48;2;0;182;0mLionel\u001b[0m\u001b[48;2;0;255;0mMessi\u001b[0m\u001b[48;2;0;211;0m的生日\u001b[0m\u001b[48;2;0;199;0m是\u001b[0m\u001b[48;2;0;200;0m1\u001b[0m\u001b[48;2;0;150;0m9\u001b[0m\u001b[48;2;0;150;0m8\u001b[0m\u001b[48;2;0;179;0m7\u001b[0m\u001b[48;2;0;200;0m年\u001b[0m\u001b[48;2;0;208;0m6\u001b[0m\u001b[48;2;0;198;0m月\u001b[0m\u001b[48;2;0;212;0m2\u001b[0m\u001b[48;2;0;159;0m4\u001b[0m\u001b[48;2;0;194;0m日\u001b[0m\u001b[48;2;0;180;0m。\u001b[0m\u001b[48;2;0;177;0m\u001b[0m\n",
      "measure by mc_enctopy_stddev\n",
      "\u001b[48;2;0;239;0mLionel\u001b[0m\u001b[48;2;0;150;0mMessi\u001b[0m\u001b[48;2;0;188;0m的生日\u001b[0m\u001b[48;2;0;255;0m是\u001b[0m\u001b[48;2;0;189;0m1\u001b[0m\u001b[48;2;0;207;0m9\u001b[0m\u001b[48;2;0;158;0m8\u001b[0m\u001b[48;2;0;226;0m7\u001b[0m\u001b[48;2;0;202;0m年\u001b[0m\u001b[48;2;0;229;0m6\u001b[0m\u001b[48;2;0;174;0m月\u001b[0m\u001b[48;2;0;168;0m2\u001b[0m\u001b[48;2;0;220;0m4\u001b[0m\u001b[48;2;0;238;0m日\u001b[0m\u001b[48;2;0;206;0m。\u001b[0m\u001b[48;2;0;189;0m\u001b[0m\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 measure by mc_stddev\n",
      "\u001b[48;2;0;250;0m\u001b[0m\u001b[48;2;0;150;0m周\u001b[0m\u001b[48;2;0;151;0m树\u001b[0m\u001b[48;2;0;164;0m人\u001b[0m\u001b[48;2;0;150;0m(\u001b[0m\u001b[48;2;0;150;0m1\u001b[0m\u001b[48;2;0;167;0m8\u001b[0m\u001b[48;2;0;150;0m8\u001b[0m\u001b[48;2;0;150;0m1\u001b[0m\u001b[48;2;0;165;0m年\u001b[0m\u001b[48;2;0;150;0m9\u001b[0m\u001b[48;2;0;255;0m月\u001b[0m\u001b[48;2;0;151;0m2\u001b[0m\u001b[48;2;0;150;0m5\u001b[0m\u001b[48;2;0;210;0m日\u001b[0m\u001b[48;2;0;163;0m-\u001b[0m\u001b[48;2;0;192;0m1\u001b[0m\u001b[48;2;0;176;0m9\u001b[0m\u001b[48;2;0;150;0m3\u001b[0m\u001b[48;2;0;156;0m6\u001b[0m\u001b[48;2;0;205;0m年\u001b[0m\u001b[48;2;0;166;0m1\u001b[0m\u001b[48;2;0;199;0m0\u001b[0m\u001b[48;2;0;159;0m月\u001b[0m\u001b[48;2;0;152;0m1\u001b[0m\u001b[48;2;0;176;0m9\u001b[0m\u001b[48;2;0;165;0m日\u001b[0m\u001b[48;2;0;184;0m)\u001b[0m\u001b[48;2;0;150;0m是中国\u001b[0m\u001b[48;2;0;150;0m著名的\u001b[0m\u001b[48;2;0;150;0m现代\u001b[0m\u001b[48;2;0;165;0m作家\u001b[0m\n",
      "measure by mc_enctopy_average\n",
      "\u001b[48;2;0;174;0m\u001b[0m\u001b[48;2;0;206;0m周\u001b[0m\u001b[48;2;0;223;0m树\u001b[0m\u001b[48;2;0;241;0m人\u001b[0m\u001b[48;2;0;219;0m(\u001b[0m\u001b[48;2;0;255;0m1\u001b[0m\u001b[48;2;0;150;0m8\u001b[0m\u001b[48;2;0;203;0m8\u001b[0m\u001b[48;2;0;213;0m1\u001b[0m\u001b[48;2;0;216;0m年\u001b[0m\u001b[48;2;0;214;0m9\u001b[0m\u001b[48;2;0;204;0m月\u001b[0m\u001b[48;2;0;220;0m2\u001b[0m\u001b[48;2;0;168;0m5\u001b[0m\u001b[48;2;0;219;0m日\u001b[0m\u001b[48;2;0;194;0m-\u001b[0m\u001b[48;2;0;193;0m1\u001b[0m\u001b[48;2;0;161;0m9\u001b[0m\u001b[48;2;0;211;0m3\u001b[0m\u001b[48;2;0;183;0m6\u001b[0m\u001b[48;2;0;180;0m年\u001b[0m\u001b[48;2;0;202;0m1\u001b[0m\u001b[48;2;0;165;0m0\u001b[0m\u001b[48;2;0;222;0m月\u001b[0m\u001b[48;2;0;164;0m1\u001b[0m\u001b[48;2;0;181;0m9\u001b[0m\u001b[48;2;0;203;0m日\u001b[0m\u001b[48;2;0;168;0m)\u001b[0m\u001b[48;2;0;233;0m是中国\u001b[0m\u001b[48;2;0;246;0m著名的\u001b[0m\u001b[48;2;0;250;0m现代\u001b[0m\u001b[48;2;0;232;0m作家\u001b[0m\n",
      "measure by mc_enctopy_stddev\n",
      "\u001b[48;2;0;232;0m\u001b[0m\u001b[48;2;0;195;0m周\u001b[0m\u001b[48;2;0;160;0m树\u001b[0m\u001b[48;2;0;170;0m人\u001b[0m\u001b[48;2;0;191;0m(\u001b[0m\u001b[48;2;0;153;0m1\u001b[0m\u001b[48;2;0;170;0m8\u001b[0m\u001b[48;2;0;185;0m8\u001b[0m\u001b[48;2;0;198;0m1\u001b[0m\u001b[48;2;0;166;0m年\u001b[0m\u001b[48;2;0;179;0m9\u001b[0m\u001b[48;2;0;215;0m月\u001b[0m\u001b[48;2;0;176;0m2\u001b[0m\u001b[48;2;0;255;0m5\u001b[0m\u001b[48;2;0;194;0m日\u001b[0m\u001b[48;2;0;183;0m-\u001b[0m\u001b[48;2;0;216;0m1\u001b[0m\u001b[48;2;0;173;0m9\u001b[0m\u001b[48;2;0;184;0m3\u001b[0m\u001b[48;2;0;189;0m6\u001b[0m\u001b[48;2;0;191;0m年\u001b[0m\u001b[48;2;0;171;0m1\u001b[0m\u001b[48;2;0;185;0m0\u001b[0m\u001b[48;2;0;178;0m月\u001b[0m\u001b[48;2;0;182;0m1\u001b[0m\u001b[48;2;0;192;0m9\u001b[0m\u001b[48;2;0;180;0m日\u001b[0m\u001b[48;2;0;209;0m)\u001b[0m\u001b[48;2;0;168;0m是中国\u001b[0m\u001b[48;2;0;150;0m著名的\u001b[0m\u001b[48;2;0;152;0m现代\u001b[0m\u001b[48;2;0;193;0m作家\u001b[0m\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 measure by mc_stddev\n",
      "\u001b[48;2;0;150;0m法国\u001b[0m\u001b[48;2;0;150;0m国旗\u001b[0m\u001b[48;2;0;150;0m是\u001b[0m\u001b[48;2;0;150;0m一面\u001b[0m\u001b[48;2;0;150;0m由\u001b[0m\u001b[48;2;0;150;0m五\u001b[0m\u001b[48;2;0;150;0m根\u001b[0m\u001b[48;2;0;150;0m垂直\u001b[0m\u001b[48;2;0;229;0m的\u001b[0m\u001b[48;2;0;150;0m蓝色\u001b[0m\u001b[48;2;0;150;0m和\u001b[0m\u001b[48;2;0;158;0m白色\u001b[0m\u001b[48;2;0;150;0m条纹\u001b[0m\u001b[48;2;0;151;0m组成的\u001b[0m\u001b[48;2;0;150;0m旗帜\u001b[0m\u001b[48;2;0;206;0m。\u001b[0m\u001b[48;2;0;150;0m蓝色\u001b[0m\u001b[48;2;0;150;0m代表\u001b[0m\u001b[48;2;0;150;0m海洋\u001b[0m\u001b[48;2;0;153;0m和\u001b[0m\u001b[48;2;0;150;0m天空\u001b[0m\u001b[48;2;0;176;0m,\u001b[0m\u001b[48;2;0;150;0m白色\u001b[0m\u001b[48;2;0;221;0m代表\u001b[0m\u001b[48;2;0;150;0m纯洁\u001b[0m\u001b[48;2;0;255;0m和\u001b[0m\u001b[48;2;0;150;0m和平\u001b[0m\u001b[48;2;0;204;0m。\u001b[0m\u001b[48;2;0;150;0m国旗\u001b[0m\u001b[48;2;0;150;0m上的\u001b[0m\u001b[48;2;0;150;0m五\u001b[0m\u001b[48;2;0;150;0m根\u001b[0m\n",
      "measure by mc_enctopy_average\n",
      "\u001b[48;2;0;150;0m法国\u001b[0m\u001b[48;2;0;241;0m国旗\u001b[0m\u001b[48;2;0;228;0m是\u001b[0m\u001b[48;2;0;235;0m一面\u001b[0m\u001b[48;2;0;239;0m由\u001b[0m\u001b[48;2;0;245;0m五\u001b[0m\u001b[48;2;0;244;0m根\u001b[0m\u001b[48;2;0;237;0m垂直\u001b[0m\u001b[48;2;0;225;0m的\u001b[0m\u001b[48;2;0;216;0m蓝色\u001b[0m\u001b[48;2;0;242;0m和\u001b[0m\u001b[48;2;0;231;0m白色\u001b[0m\u001b[48;2;0;241;0m条纹\u001b[0m\u001b[48;2;0;210;0m组成的\u001b[0m\u001b[48;2;0;216;0m旗帜\u001b[0m\u001b[48;2;0;192;0m。\u001b[0m\u001b[48;2;0;205;0m蓝色\u001b[0m\u001b[48;2;0;218;0m代表\u001b[0m\u001b[48;2;0;238;0m海洋\u001b[0m\u001b[48;2;0;235;0m和\u001b[0m\u001b[48;2;0;255;0m天空\u001b[0m\u001b[48;2;0;232;0m,\u001b[0m\u001b[48;2;0;244;0m白色\u001b[0m\u001b[48;2;0;249;0m代表\u001b[0m\u001b[48;2;0;234;0m纯洁\u001b[0m\u001b[48;2;0;198;0m和\u001b[0m\u001b[48;2;0;239;0m和平\u001b[0m\u001b[48;2;0;222;0m。\u001b[0m\u001b[48;2;0;221;0m国旗\u001b[0m\u001b[48;2;0;227;0m上的\u001b[0m\u001b[48;2;0;223;0m五\u001b[0m\u001b[48;2;0;246;0m根\u001b[0m\n",
      "measure by mc_enctopy_stddev\n",
      "\u001b[48;2;0;255;0m法国\u001b[0m\u001b[48;2;0;162;0m国旗\u001b[0m\u001b[48;2;0;178;0m是\u001b[0m\u001b[48;2;0;203;0m一面\u001b[0m\u001b[48;2;0;166;0m由\u001b[0m\u001b[48;2;0;163;0m五\u001b[0m\u001b[48;2;0;176;0m根\u001b[0m\u001b[48;2;0;153;0m垂直\u001b[0m\u001b[48;2;0;194;0m的\u001b[0m\u001b[48;2;0;207;0m蓝色\u001b[0m\u001b[48;2;0;171;0m和\u001b[0m\u001b[48;2;0;160;0m白色\u001b[0m\u001b[48;2;0;150;0m条纹\u001b[0m\u001b[48;2;0;194;0m组成的\u001b[0m\u001b[48;2;0;197;0m旗帜\u001b[0m\u001b[48;2;0;210;0m。\u001b[0m\u001b[48;2;0;242;0m蓝色\u001b[0m\u001b[48;2;0;194;0m代表\u001b[0m\u001b[48;2;0;159;0m海洋\u001b[0m\u001b[48;2;0;156;0m和\u001b[0m\u001b[48;2;0;153;0m天空\u001b[0m\u001b[48;2;0;171;0m,\u001b[0m\u001b[48;2;0;168;0m白色\u001b[0m\u001b[48;2;0;203;0m代表\u001b[0m\u001b[48;2;0;161;0m纯洁\u001b[0m\u001b[48;2;0;210;0m和\u001b[0m\u001b[48;2;0;157;0m和平\u001b[0m\u001b[48;2;0;198;0m。\u001b[0m\u001b[48;2;0;183;0m国旗\u001b[0m\u001b[48;2;0;205;0m上的\u001b[0m\u001b[48;2;0;201;0m五\u001b[0m\u001b[48;2;0;163;0m根\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"梅西的生日是？\",\n",
    "    \"周树人的生日是？\",\n",
    "    \"简要描述法国国旗\"\n",
    "]\n",
    "\n",
    "with open(\"result.pkl\", 'wb') as f:\n",
    "    for prompt in prompts:\n",
    "        outputs = generate(prompt, tokenizer, model, forward_n=FORWARD_N)\n",
    "        record = {\"text\": prompt, \"outputs\": outputs}\n",
    "        print(\"measure by mc_stddev\")\n",
    "        print_seq([tokenizer.decode(tk) for tk in outputs[0]], outputs[1])\n",
    "        print(\"measure by mc_enctopy_average\")\n",
    "        print_seq([tokenizer.decode(tk) for tk in outputs[0]], outputs[2])\n",
    "        print(\"measure by mc_enctopy_stddev\")\n",
    "        print_seq([tokenizer.decode(tk) for tk in outputs[0]], outputs[3])\n",
    "        pkl.dump(record, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc66a16",
   "metadata": {},
   "source": [
    "受限计算资源，长度限制在32\n",
    "\n",
    "background color代表句子内的相对确定程度，在大量真实语料下，可以计算语料内所有token的相关指标，挑选合适的作为评判标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d23847",
   "metadata": {},
   "source": [
    "### What to do\n",
    "基于统计学不确定性估计计算token真实性指标，优点在于无监督，有理论支持，实现方便，后续扩展方便（token惩罚、设计sentence真实性、不确定性干预训练过程等）；缺点在于真实场景下，需要一个“指标的指标”来评估效果。\n",
    "\n",
    "hallucinate的问题是难以定义，这也是相关研究大都自建数据集，标注，评估效果的原因。在实际场景中，先要明确定义什么是”hallucinate“，例如和结构化的知识库不一致，和prompt中提供的信息不一致，基于明确定义才能有后续的优化目标。\n",
    "\n",
    "信息提取可能是改善hallucinate的问题途径之一，可以在训练过程中，抽取entity，替换错误结果，引导reward模型识别错误；在推理过程中抽取关键entity，计算不确定性等等（大部分token不是关键信息）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52048962",
   "metadata": {},
   "source": [
    "### Reference & Notes\n",
    "\n",
    "-  [Survey of Hallucination in Natural Language Generation](https://arxiv.org/pdf/2202.03629.pdf) (2022)\n",
    "   -  Measurement (section 4)\n",
    "      -  statistic-based，与语料越“相似”约可信，用gram-based、vector-based度量相似性\n",
    "      -  model-based\n",
    "         -  信息抽取，比较三元组\n",
    "         -  Qa-based，反复问，看回答是否一致（不确定的答案导致解码过程倾向于随机token从而导致不一致）\n",
    "         -  NLI-based，外部模型打分，看回答和语料是否“entailment”\n",
    "         -  LM-based，一个只在summarization上训练的LM0和一个在source及summarization训练的LM1，比较token的概率，小于LM1时认为是虚假信息，设计整段文本的指标\n",
    "   -  Method (section 5)\n",
    "      -  better dataset (clean, argument...)\n",
    "      -  model architecture\n",
    "         -  Encoder .... (skipped because gpt use decoder only)\n",
    "         -  **Decoder （问答是one-to-many问题，核心想法是检测多次回答的一致性）**\n",
    "            -  [ ] multi-branch\n",
    "            -  [ ] Uncertainty-aware\n",
    "            -  [ ] dual\n",
    "            -  [ ] tree-based\n",
    "            -  [ ] ... \n",
    "\n",
    "-  [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/pdf/2303.08896.pdf) (2023)\n",
    "   -  method\n",
    "      -  method1: 确定的答案收束，错误的答案发散，在有token prob的情况下，计算 f = sum(ppl(r))/|r| 代表信息真实程度。\n",
    "      -  method2: 基于BertScore。设计了类似列表相似度的指标，通过beamsearch生成的多个答案，根据句子相似度计算“不可靠”程度。\n",
    "      -  method3: Qa-based...\n",
    "\n",
    "\n",
    "\n",
    "-  [A Token-level Reference-free Hallucination Detection Benchmark forFree-form Text Generation](https://arxiv.org/pdf/2104.08704.pdf) (2022)\n",
    "   - token level classification based on created dataset\n",
    "\n",
    "-  [Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation](https://arxiv.org/abs/2208.05309)\n",
    "\n",
    "-  [UNCERTAINTY ESTIMATION IN AUTOREGRESSIVE STRUCTURED PREDICTION](https://arxiv.org/pdf/2002.07650.pdf)\n",
    "\n",
    "-  [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://proceedings.mlr.press/v48/gal16.pdf)\n",
    "\n",
    "- [monte-carlo-dropout](https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
